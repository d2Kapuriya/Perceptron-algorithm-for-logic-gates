{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXlJ933-SGZw"
   },
   "source": [
    "# Perceptron algorithm for logic gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLIGPcxeSGZ1"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oT7xJ6LzSGaE"
   },
   "source": [
    "The computational graph of our perceptron is:\n",
    "<img src=\"perceptron1.png\" height=\"400\" width=\"200\" />\n",
    "The Σ symbol represents the linear combination of the inputs x by means of the weights w and the bias b. Since this notation is quite heavy, from now on I will simplify the computational graph in the following way:\n",
    "<img src=\"perceptron2.png\" height=\"400\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyN9afg7SGaG"
   },
   "outputs": [],
   "source": [
    "def unit_step(v):\n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def perceptron(x, w, b):\n",
    "    v = np.dot(w, x) + b\n",
    "    y = unit_step(v)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ou5ciQsVSGaS"
   },
   "source": [
    "**NOT logical function**\n",
    "\n",
    "NOT(x) is a 1-variable function, that means that we will have one input at a time: N=1. Also, it is a logical function, and so both the input and the output have only two possible states: 0 and 1 (i.e., False and True): the Heaviside step function seems to fit our case since it produces a binary output.\n",
    "<img src=\"not.png\" height=\"50\" width=\"50\" />\n",
    "The fundamental question is: do exist two values that, if picked as parameters, allow the perceptron to implement the NOT logical function? When I say that a perceptron implements a function, I mean that for each input in the function’s domain the perceptron returns the same number (or vector) the function would return for the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4su1IK9SGaU"
   },
   "outputs": [],
   "source": [
    "def NOT_percep(x):    \n",
    "    return perceptron(x, w=-1, b=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cb3dKfwCSGaf"
   },
   "source": [
    "**AND logical function**\n",
    "<img src=\"and.png\" height=\"100\" width=\"100\" />\n",
    "\n",
    "The AND logical function is a 2-variables function, AND(x1, x2), with binary inputs and output.\n",
    "This graph is associated with the following computation:\n",
    "ŷ = ϴ(w1*x1 + w2*x2 + b)\n",
    "This time, we have three parameters: w1, w2, and b.\n",
    "w1 = 1, w2 = 1, b = -1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NN0yRAxaSGah"
   },
   "outputs": [],
   "source": [
    "def AND_percep(x1,x2):\n",
    "    w = np.array([1, 1])\n",
    "    b =-1.5\n",
    "    x = np.array([x1,x2])\n",
    "    return perceptron(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQcjX1qvSGas"
   },
   "source": [
    "**OR logical function**\n",
    "\n",
    "OR(x1, x2) is a 2-variables function too, and its output is 1-dimensional (i.e., one number) and has two possible states (0 or 1). Therefore, we will use a perceptron with the same architecture as the one before. \n",
    "w1 = 1, w2 = 1, b = -0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bl3TIwISGau"
   },
   "outputs": [],
   "source": [
    "def OR_percep(x1,x2):\n",
    "    w = np.array([1, 1])\n",
    "    b =-0.5\n",
    "    x = np.array([x1,x2])\n",
    "    return perceptron(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djkUISG1SGa1"
   },
   "source": [
    "**XOR logical function**\n",
    "<img src=\"xor1.png\" height=\"100\" width=\"400\"/>\n",
    "<img src=\"xor2.png\" height=\"100\" width=\"100\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NtJKUFCSGa3"
   },
   "outputs": [],
   "source": [
    "def XOR_net(x1,x2):\n",
    "    out_1 = AND_percep(x1,x2)\n",
    "    out_2 = NOT_percep(out_1)\n",
    "    out_3 = OR_percep(x1,x2)\n",
    "    output = AND_percep(out_2,out_3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DbrDKOzSGa9",
    "outputId": "d41aeb65-fd2d-4061-b147-5153c67be4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT(0) = 1\n",
      "NOT(1) = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NOT(0) = {}\".format(NOT_percep(0)))\n",
    "print(\"NOT(1) = {}\".format(NOT_percep(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1360,
     "status": "ok",
     "timestamp": 1587013800753,
     "user": {
      "displayName": "DIXIT KAPURIYA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyeyTmWpD8ty95MkLD3zxUyzaBl1_Oj8QU_Cdc=s64",
      "userId": "10373406633725039374"
     },
     "user_tz": -330
    },
    "id": "oYCmaUV3SGbD",
    "outputId": "16b6ed1c-1957-467d-b9ce-c29a8871a2ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND(1, 1) = 1\n",
      "AND(1, 0) = 0\n",
      "AND(0, 1) = 0\n",
      "AND(0, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"AND({}, {}) = {}\".format(1, 1, AND_percep(1,1)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 0, AND_percep(1,0)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 1, AND_percep(0,1)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 0, AND_percep(0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1493,
     "status": "ok",
     "timestamp": 1587013803872,
     "user": {
      "displayName": "DIXIT KAPURIYA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyeyTmWpD8ty95MkLD3zxUyzaBl1_Oj8QU_Cdc=s64",
      "userId": "10373406633725039374"
     },
     "user_tz": -330
    },
    "id": "3NV-0SPxSGbH",
    "outputId": "09618738-dce1-4f73-a1ea-e84e6b81fe76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR(1, 1) = 1\n",
      "OR(1, 0) = 1\n",
      "OR(0, 1) = 1\n",
      "OR(0, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"OR({}, {}) = {}\".format(1, 1, OR_percep(1,1)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 0, OR_percep(1,0)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, OR_percep(0,1)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 0, OR_percep(0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1559,
     "status": "ok",
     "timestamp": 1587013807404,
     "user": {
      "displayName": "DIXIT KAPURIYA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgyeyTmWpD8ty95MkLD3zxUyzaBl1_Oj8QU_Cdc=s64",
      "userId": "10373406633725039374"
     },
     "user_tz": -330
    },
    "id": "zVDFs9cVSGbL",
    "outputId": "43f07611-fe84-46c5-c9f8-1d9fdb72a175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR(1, 1) = 0\n",
      "OR(1, 0) = 1\n",
      "OR(0, 1) = 1\n",
      "OR(0, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"OR({}, {}) = {}\".format(1, 1, XOR_net(1,1)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 0, XOR_net(1,0)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, XOR_net(0,1)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 0, XOR_net(0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFopTOtqSGbP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Perceptron algorithm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
